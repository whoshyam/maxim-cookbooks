{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LiteLLM Integration with Maxim SDK Tracing\n",
    "\n",
    "This tutorial demonstrates how to integrate Maxim SDK tracing capabilities with LiteLLM. You'll learn how to set up and configure Maxim's tracing functionality to monitor and analyze your LiteLLM API calls, providing valuable insights into your LLM application's performance and usage patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Initializing Maxim SDK and adding as a logger in litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "from maxim import Maxim\n",
    "from maxim.logger.litellm import MaximLiteLLMTracer\n",
    "\n",
    "logger = Maxim().logger()\n",
    "# This is the single line integration of Maxim SDK with LiteLLM\n",
    "litellm.callbacks = [MaximLiteLLMTracer(logger)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Initializing litellm router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from litellm import acompletion\n",
    "\n",
    "response = await acompletion(\n",
    "        model=\"openai/gpt-4o\",\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Hello, world!\"}],        \n",
    "    )\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attaching a custom trace to the litellm generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maxim.logger.logger import TraceConfig\n",
    "import uuid\n",
    "\n",
    "trace = logger.trace(TraceConfig(id=str(uuid.uuid4()), name=\"litellm-generation\"))\n",
    "trace.event(str(uuid.uuid4()), \"litellm-generation\", \"litellm-generation\", {})\n",
    "# This will attach litellm generation to the trace inside a span.\n",
    "response = await acompletion(\n",
    "        model=\"openai/gpt-4o\",\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        messages=[{\"role\": \"user\", \"content\": \"What can you do for me!\"}],        \n",
    "        metadata={\"maxim\": {\"trace_id\": trace.id, \"span_name\": \"litellm-generation\"}}\n",
    "    )\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![LiteLLM Custom Trace](images/litellm-custom-trace.png)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
