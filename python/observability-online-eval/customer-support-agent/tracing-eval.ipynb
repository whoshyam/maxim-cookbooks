{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logging and Evaluation your Customer Support Agent using Maxim SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the tutorial on how to use the [Maxim python SDK](https://www.getmaxim.ai/docs/sdk/observability/python/overview) to easily log and evaluate your Customer Support agent using decorators.\n",
    "\n",
    "We hope you have read the guide [here](https://github.com/maximhq/maxim-cookbooks/tree/main/python/simulation/simulation_workflow.md) and are familiar with the basic structure of the agent.\n",
    "\n",
    "Let's head on to the logging and evaluation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Learning Objectives**\n",
    "\n",
    "By the end of this tutorial, you will be able to:\n",
    "- Understand the basics of tracing using Maxim SDK.\n",
    "- Use the SDK to trace your customer support agent.\n",
    "- Evaluate the performance of your agent using Maxim's built-in evaluators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from uuid import uuid4\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "from src.llm_agent import LLMAgent\n",
    "from src.embeddings import EmbeddingManager\n",
    "\n",
    "from maxim import Maxim, Config\n",
    "from maxim.logger import LoggerConfig, SessionConfig\n",
    "from maxim.decorators import trace, current_trace, retrieval, current_retrieval, generation, current_generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of this tutorial and modular systems, distributed tracing might be good to understand, refer to this for the same: https://www.getmaxim.ai/docs/observability/quickstart/distributed-tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxim_api_key = \"<your-maxim-api-key>\"\n",
    "maxim_log_repository_id = \"<your-maxim-log-repository-id>\"\n",
    "\n",
    "maxim = Maxim(Config(api_key=maxim_api_key))\n",
    "logger = maxim.logger(LoggerConfig(id=maxim_log_repository_id))\n",
    "\n",
    "agent = LLMAgent()\n",
    "app = FastAPI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Endpoint\n",
    "Learn more about it here: https://www.getmaxim.ai/docs/observability/concepts#trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our implementation of the customer support agent, we have a `/chat` endpoint that is used to recieve user input and post processing from the agent, send the response back to the user. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few things to note:\n",
    "- The session ID is the same as the conversation ID, which is either sent along with the request or is randomly generated during runtime.\n",
    "- A session captures an entire conversation, while a trace focuses more on a single complete turn of the conversation. You can learn more about them here: https://www.getmaxim.ai/docs/observability/concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatRequest(BaseModel):\n",
    "    query: str\n",
    "    conversation_id: Optional[str] = None\n",
    "\n",
    "@trace(logger=logger, id=str(uuid4()), sessionId=lambda request: request.conversation_id, name=\"chat-endpoint\", evaluators=['toxicity', 'bias'])\n",
    "@app.post('/chat')\n",
    "async def chat(\n",
    "    request: ChatRequest,\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Endpoint to handle chat requests.\n",
    "    \"\"\"\n",
    "    # Defining variables\n",
    "    conversation_id = request.conversation_id or str(uuid4())\n",
    "    query = request.query\n",
    "\n",
    "    # Initializing Session\n",
    "    session_name = f\"Session ID: {conversation_id}\"\n",
    "    session = logger.session(SessionConfig(id=conversation_id, name=session_name))\n",
    "\n",
    "    current_trace().set_input(query)\n",
    "    # Calling the main agent function\n",
    "    response = agent.customer_support_pipeline(query, conversation_id)\n",
    "    current_trace().set_output(response)\n",
    "    current_trace().evaluate().with_variables({'output': response.content}, ['toxicity', 'bias'])\n",
    "    \n",
    "    # Once completed, we can end the trace, session and cleanup the logger\n",
    "    current_trace().end()\n",
    "    session.end()\n",
    "    logger.cleanup()\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capturing Components\n",
    "Learn more about retrieval here: https://www.getmaxim.ai/docs/observability/concepts#retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A customer support agent requires a knowledge base for context specific answers. The knowledge base in this case are the customer, product and order details, along with policy documents.\n",
    "The agents needs to be able to retrieve the relevant sections of the base in order to answer user queries effectively.\n",
    "Here is how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMAgent:\n",
    "   @retrieval(logger=logger, id=str(uuid4()), name=\"knowledge-retrieval\", evaluators=[\"context relevance\", \"output relevance\"])\n",
    "   def customer_support_pipeline(\n",
    "        self, \n",
    "        user_query: str,\n",
    "        conversation_id: str,\n",
    "        num_chunks: int = 3\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Complete support pipeline with conversation persistence.\"\"\"\n",
    "\n",
    "        # Get response from retrieval pipeline (returns a list of tuples)\n",
    "        retrieved_chunks = EmbeddingManager.search(user_query, k=num_chunks)\n",
    "\n",
    "        current_retrieval().input(user_query)\n",
    "        retrieved_chunks_log = [str(chunk) for chunk in retrieved_chunks_log]\n",
    "        current_retrieval().output(retrieved_chunks_log)\n",
    "        \n",
    "        retrieved_chunks_log = \"\\n\\n\".join(retrieved_chunks)\n",
    "        current_retrieval().evaluate().with_variables({'input': user_query, 'context': retrieved_chunks_log}, ['context relevance'])\n",
    "\n",
    "        message_history = None       \n",
    "        # load conversation into message_history using conversation_id for example     \n",
    "        # generate response\n",
    "\n",
    "        latest_response = self.generate_response(message_history)\n",
    "        current_retrieval().evaluate().with_variables({'input': user_query, 'output': latest_response}, ['output relevance'])\n",
    "        current_retrieval().end()\n",
    "\n",
    "        # save conversation\n",
    "        return latest_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM generation\n",
    "Learn more about generation here: https://www.getmaxim.ai/docs/observability/concepts#generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most important things to log is the LLM generation to a user query. \n",
    "Using generation decorator, we can also evaluate the agent's performance and identify issues underlying the LLM's behavior. \n",
    "\n",
    "A few things to keep in mind:\n",
    "- While we in this example use OpenAI, you can use any LLM model of choice that supports function calls.\n",
    "- These are code templates to help out, implementation often varies from system to system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMAgent:\n",
    "    @generation(logger=logger, id=str(uuid4()), name=\"openai-generation\", evaluators=['toxicity', 'bias'])\n",
    "    def generate_response(\n",
    "            self, \n",
    "            messages: List[Dict],\n",
    "        ) -> List[Dict]:\n",
    "            \"\"\"Generate a response using GPT-4 with retrieved context and handle tool calls.\"\"\"\n",
    "\n",
    "            # Initialize the generation (Learn more about generation here: )\n",
    "            current_generation().provider(\"openai\")\n",
    "            current_generation().model(\"gpt-4o\")\n",
    "            # obtain a response from the model (remember to pass the tools)\n",
    "            response = openai.chat.completions.create()\n",
    "\n",
    "            current_generation().result(response)\n",
    "            # Attaching evaluators to the generation\n",
    "            current_generation().evaluate().with_variables({'output': response.content}, ['toxicity', 'bias'])\n",
    "            current_generation().end()\n",
    "\n",
    "\n",
    "            return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool Calls\n",
    "Learn more about tool call here: https://www.getmaxim.ai/docs/observability/concepts#tool-call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important part of any agent's work is the tool calls being made. These are the critical points where most agents tend to fail, logging them and being able evaluate comes as a big part of the improvement process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continuing from the previous codeblock\n",
    "class LLMAgent:\n",
    "    @tool_call(logger=logger, id=str(uuid4()), name=\"tool_call_name\", description=\"tool_call_description\", args=\"tool_call_args\", evaluators=['toxicity', 'bias'])\n",
    "    def generate_response(\n",
    "            self, \n",
    "            messages: List[Dict],\n",
    "        ) -> List[Dict]:\n",
    "            \"\"\"Generate a response using GPT-4 with retrieved context and handle tool calls.\"\"\"\n",
    "            \n",
    "            response = openai.chat.completions.create()\n",
    "\n",
    "            # Initialize the tool call configuration\n",
    "            tool_call = response.tool_calls[0]\n",
    "            tool_call_name = tool_call.function.name\n",
    "            tool_call_args = tool_call.function.arguments\n",
    "            tool_call_description = \"\" # Place a tool call description here\n",
    "\n",
    "            # Perform tool calling as necessary\n",
    "            tool_result = self.execute_tool(tool_call)\n",
    "\n",
    "            # Log the tool call result obtained\n",
    "            current_tool_call().result(tool_result)\n",
    "            current_tool_call().end()\n",
    "\n",
    "            return tool_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on reaching the end of this guide! ðŸš€ By now, you should have a solid integration of the Maxim SDK into your project and should start seeing the results on the platform in your log repository.\n",
    "To learn more about the Maxim SDK, checkout the Maxim SDK documentation [here](https://www.getmaxim.ai/docs/sdk/overview).\n",
    "\n",
    "Happy tracing!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
