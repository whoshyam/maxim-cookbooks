{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Maxim SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from dotenv import dotenv_values\n",
    "from maxim import Config, Maxim\n",
    "from maxim.evaluators import BaseEvaluator\n",
    "from maxim.models import (\n",
    "    LocalEvaluatorResultParameter,\n",
    "    LocalEvaluatorReturn,\n",
    "    ManualData,\n",
    "    PassFailCriteria,\n",
    "    YieldedOutput\n",
    ")\n",
    "from maxim.models.evaluator import (\n",
    "    PassFailCriteriaForTestrunOverall,\n",
    "    PassFailCriteriaOnEachEntry,\n",
    ")\n",
    "\n",
    "config = dotenv_values()\n",
    "\n",
    "API_KEY: str = config.get(\"MAXIM_API_KEY\") or \"\"\n",
    "WORKSPACE_ID: str = config.get(\"MAXIM_WORKSPACE_ID\") or \"\"\n",
    "WORKFLOW_ID: str = config.get(\"MAXIM_WORKFLOW_ID\") or \"\"\n",
    "DATASET_ID: str = config.get(\"MAXIM_DATASET_ID\") or \"\"\n",
    "PROMPT_VERSION_ID: str = config.get(\"MAXIM_PROMPT_VERSION_ID\") or \"\"\n",
    "MAXIM_UNKNOWN_WORKFLOW_ID: str = config.get(\"MAXIM_UNKNOWN_WORKFLOW_ID\") or \"\"\n",
    "MAXIM_INVALID_WORKFLOW_ID: str = config.get(\"MAXIM_INVALID_WORKFLOW_ID\") or \"\"\n",
    "\n",
    "maxim = Maxim(\n",
    "    config=Config(\n",
    "        api_key=API_KEY,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define local workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(data: ManualData):\n",
    "    \"\"\"\n",
    "    This will contain you local workflow.\n",
    "    For this cookbook, we are sending hardcoded test as output\n",
    "    YieldedOutput type also supports metadata like\n",
    "        - meta\n",
    "            - cost\n",
    "            - token usage etc.\n",
    "    You can also pass context as retrieved_context_to_evaluate\n",
    "    \"\"\"\n",
    "    print(f\"processing => {data.get(\"Input\")}\")\n",
    "    return YieldedOutput(data=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define custom evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomEvaluator(BaseEvaluator):\n",
    "    \"\"\"\n",
    "    Custom evaluator class that extends BaseEvaluator to perform custom evaluations.\n",
    "    \"\"\"\n",
    "\n",
    "    def evaluate(\n",
    "        self, result: LocalEvaluatorResultParameter, data: ManualData\n",
    "    ) -> Dict[str, LocalEvaluatorReturn]:\n",
    "        \"\"\"\n",
    "        You are supposed to override this function and run evaluations\n",
    "        Args:\n",
    "            result (LocalEvaluatorResultParameter): The result parameter containing evaluation data\n",
    "            data (ManualData): The manual data to evaluate against\n",
    "        Returns:\n",
    "            Dict[str, LocalEvaluatorReturn]: Dictionary mapping evaluation names to score results.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"Evaluation 1\": LocalEvaluatorReturn(score=1),\n",
    "            \"Evaluation 2\": LocalEvaluatorReturn(score=False, reasoning=\"Just chillll\"),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and trigger test run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxim.create_test_run(name=\"Local workflow test run from SDK\", in_workspace_id=WORKSPACE_ID).with_data(\n",
    "    DATASET_ID\n",
    ").with_concurrency(2).with_evaluators(\n",
    "    \"Bias\",\n",
    "    MyCustomEvaluator(\n",
    "        pass_fail_criteria={\n",
    "            \"Evaluation 1\": PassFailCriteria(\n",
    "                for_testrun_overall_pass_if=PassFailCriteriaForTestrunOverall(\n",
    "                    \">\", 3, \"average\"\n",
    "                ),\n",
    "                on_each_entry_pass_if=PassFailCriteriaOnEachEntry(\">\", 1),\n",
    "            ),\n",
    "            \"Evaluation 2\": PassFailCriteria(\n",
    "                for_testrun_overall_pass_if=PassFailCriteriaForTestrunOverall(\n",
    "                    overall_should_be=\"!=\", value=2, for_result=\"average\"\n",
    "                ),\n",
    "                on_each_entry_pass_if=PassFailCriteriaOnEachEntry(\"=\", True),\n",
    "            ),\n",
    "        }\n",
    "    ),\n",
    ").yields_output(run).run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
